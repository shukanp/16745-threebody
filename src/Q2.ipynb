{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.activate(joinpath(@__DIR__,\"..\")); Pkg.instantiate();\n",
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "if !@isdefined autograder\n",
    "    using Plots\n",
    "end\n",
    "using Test\n",
    "include(joinpath(@__DIR__,\"utils.jl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-neutral",
   "metadata": {},
   "source": [
    "# Q2: Newton's Method (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-telling",
   "metadata": {},
   "source": [
    "## Part (a): Newton's method in 1 dimension (1pt)\n",
    "First let's look at a nonlinear function, and label where this function is equal to 0 (a root of the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "if !@isdefined autograder\n",
    "    let \n",
    "        x = 2:0.1:4;\n",
    "        y = sin.(x) .* x.^2\n",
    "        plot(x,y,label = \"function of interest\")\n",
    "        plot!(x,0*x,linestyle = :dash, color = :black,label = \"\")\n",
    "        xlabel!(\"x\")\n",
    "        ylabel!(\"f(x)\")\n",
    "        scatter!([pi],[0],label = \"zero\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-taste",
   "metadata": {},
   "source": [
    "We are now going to use Newton's method to numerically evaluate the argument $x$ where this function is equal to zero. To make this more general, let's define a residual function,\n",
    "$$ r(x) = \\sin(x)x^2. $$\n",
    "We want to drive this residual function to be zero (aka find a root to $r(x)$). To do this, we start with an initial guess at $x_k$, and approximate our residual function with a first-order Taylor expansion:\n",
    "$$ r(x_k + \\Delta x) \\approx r(x_k) + \\bigg[ \\frac{\\partial r}{\\partial x}\\bigg\\rvert_{x_k} \\bigg] \\Delta x. $$ \n",
    "We now want to find the root of this linear approximation. In other words, we want to find a $\\Delta x$ such that $r(x_k + \\Delta x) = 0$. To do this, we simply re-arrange:\n",
    "$$ \\Delta x = -\\bigg[ \\frac{\\partial r}{\\partial x}\\bigg\\rvert_{x_k} \\bigg]^{-1}r(x_k). $$ \n",
    "We can now increment our estimate of the root with the following:\n",
    "$$ x_{k+1} = x_k + \\Delta x$$\n",
    "We have now described one step of Netwon's method. We started with an initial point, linearized the residual function, and solved for the $\\Delta x$ that drove this linear approximation to zero. We keep taking Newton steps until $r(x_k)$ is close enough to zero for our purposes (usually not hard to drive below 1e-10). \n",
    "\n",
    "\n",
    "Julia tip: `x=A\\b` solves linear systems of the form $Ax = b$ whether $A$ is a matrix or a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"2a\" begin # POINTS = 1\n",
    "    # residual function \n",
    "    r(_x) = sin(_x)*_x^2\n",
    "    \n",
    "    # vectors for storing the x_k's and r(x_k)'s\n",
    "    X = NaN*ones(10)\n",
    "    R = NaN*ones(10)\n",
    "    \n",
    "    # initial point NOTE: Newton's method always needs an initial point.\n",
    "    X[1] = 2.8\n",
    "    R[1] = r(X[1])\n",
    "    \n",
    "    # TODO: use Newton's method to find the x such that r(x) = 0 \n",
    "    # store x values in X, R values in R \n",
    "    # hint: R[i] = r(X[i]) at each iteration\n",
    "    # You can exit your loop when the norm of the residual is < 1e-10\n",
    "    \n",
    "    ## SOLUTION\n",
    "    # main loop for Newton's method\n",
    "    for k = 1:9\n",
    "        \n",
    "        # TODO: use your newton_step function to take a step, then check convergence\n",
    "        # store x values in X, and store the residuals in R \n",
    "        # hint: R[i] = r(X[i])\n",
    "        \n",
    "    end\n",
    "        \n",
    "    # TODO: trim the NaN's from your X and R (here I trimmed to the first 6) \n",
    "    R = R[1:6]\n",
    "    X = X[1:6]\n",
    "    \n",
    "    # tests\n",
    "    @test norm(r(X[end]))<1e-7 #  POINTS = 1\n",
    "    \n",
    "    # plotting\n",
    "    if !@isdefined autograder\n",
    "        display(plot(norm.(R),yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "             yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "             title = \"Convergence of Newton's Method (1D case)\",label = \"\"))\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-independence",
   "metadata": {},
   "source": [
    "## Part (b): Newton's method in multiple variables (1 pt)\n",
    "We are now going to use Newton's method to solve for the zero of a multivariate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"2b\" begin # POINTS = 1\n",
    "    # residual function \n",
    "    r(x) = [sin(x[3] + 0.3)*cos(x[2]- 0.2) - 0.3*x[1];\n",
    "            cos(x[1]) + sin(x[2]) + tan(x[3]);\n",
    "            3*x[1] + 0.1*x[2]^3]\n",
    "    \n",
    "    # vectors for storing the x_k's and r(x_k)'s\n",
    "    X = NaN*[zeros(3) for i = 1:10]\n",
    "    R = NaN*[zeros(3) for i = 1:10]\n",
    "    \n",
    "    # initial point NOTE: Newton's method always needs an initial point.\n",
    "    X[1] = [.1;.1;0.1]\n",
    "    R[1] = r(X[1])\n",
    "    \n",
    "    # TODO: use Newton's Method again to find x st r(x) = zeros(3)\n",
    "    # You can exit your loop when the norm of the residual is < 1e-10\n",
    "    \n",
    "    ## Solution \n",
    "    # main loop \n",
    "    for k = 1:9\n",
    "        \n",
    "        # TODO: use your newton_step function to take a step, then check convergence\n",
    "        # store x values in X, and store the residuals in R \n",
    "        # hint: R[i] = r(X[i])\n",
    "        \n",
    "        \n",
    "    end\n",
    "        \n",
    "    # trim the NaN's away (you can change this number if you need to, but you shouldn't have to )\n",
    "    R = R[1:5]    \n",
    "    X = X[1:5]\n",
    "    Rp = [[abs(R[i][ii]) for i = 1:length(R)] for ii = 1:3] # this gets abs of each term at each iteration\n",
    "    \n",
    "    # tests \n",
    "    @test norm(r(X[end]))<1e-7 # POINTS = 1\n",
    "    \n",
    "    # convergence plotting \n",
    "    if !@isdefined autograder\n",
    "        plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "             yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "             title = \"Convergence of Newton's Method (3D case)\",label = \"|r_1|\")\n",
    "        plot!(Rp[2],label = \"|r_2|\")\n",
    "        display(plot!(Rp[3],label = \"|r_3|\"))\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-planet",
   "metadata": {},
   "source": [
    "## Part (c): Newtons method in optimization (1 pt)\n",
    "Now let's look at how we can use Newton's method in numerical optimization. Let's start by plotting a cost function $f(x)$, where $x\\in \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "    q = [2;-3]\n",
    "    f(x) = 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2) # cost function \n",
    "    contour(-1:.1:1,-1:.1:1, (x1,x2)-> f([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X_1\", ylabel = \"X_2\",fill = true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-mother",
   "metadata": {},
   "source": [
    "To find the minimum for this cost function $f(x)$, let's write the KKT conditions for optimality:\n",
    "\n",
    "\n",
    "$$ \\nabla f(x) = 0 \\quad \\quad \\text{stationarity}, $$\n",
    "\n",
    "\n",
    "which we see is just another rootfinding problem. We are now going to use Newton's method on the KKT conditions to find the $x$ in which $\\nabla f(x) = 0$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"2c\" begin            # POINTS = 1\n",
    "    Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "    q = [2;-3]\n",
    "    f(x) = 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2)\n",
    "    \n",
    "    function kkt_conditions(x)\n",
    "        \n",
    "        # TODO: return the stationarity condition for the cost function f (âˆ‡f(x))\n",
    "        \n",
    "        return NaN*zeros(2)\n",
    "    end\n",
    "    \n",
    "    # the residual function we are driving to zero \n",
    "    r(x) = kkt_conditions(x)\n",
    "    \n",
    "    # vectors for storing the x_k's and r(x_k)'s\n",
    "    X = NaN*[zeros(2) for i = 1:10]\n",
    "    R = NaN*[zeros(2) for i = 1:10]\n",
    "    \n",
    "    # initial point NOTE: Newton's method always needs an initial point.\n",
    "    X[1] = [-0.9512129986081451, 0.8061342694354091]\n",
    "    R[1] = r(X[1])\n",
    "    \n",
    "    # TODO: use Newton's Method to find x st r(x) = zeros(3)\n",
    "    # You can exit your loop when the norm of the residual is < 1e-10\n",
    "    \n",
    "    # SOLUTION\n",
    "    # main loop \n",
    "    for k = 1:9\n",
    "        \n",
    "        # TODO: use your newton_step function to take a step, then check convergence\n",
    "        # store x values in X, and store the residuals in R \n",
    "        # hint: R[i] = r(X[i])\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # trim the NaN's away (you can change this number if you need to, but you shouldn't have to )\n",
    "    R = R[1:6]\n",
    "    X = X[1:6]\n",
    "    Rp = [[abs(R[i][ii]) for i = 1:length(R)] for ii = 1:length(R[1])] # this gets abs of each term at each iteration\n",
    "    \n",
    "    # tests \n",
    "    @test norm(r(X[end]))<1e-7              # POINTS = 1\n",
    "\n",
    "    if !@isdefined autograder\n",
    "        plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "             yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "             title = \"Convergence of Newton's Method on KKT Conditions\",label = \"|r_1|\")\n",
    "        display(plot!(Rp[2],label = \"|r_2|\"))\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-scene",
   "metadata": {},
   "source": [
    "## Note on Newton's method for unconstrained optimization\n",
    "To solve the above problem, we used Newton's method on the following equation:\n",
    "\n",
    "$$ \\nabla f(x) = 0 \\quad \\quad \\text{stationarity}, $$\n",
    "\n",
    "Which results in the following Newton steps:\n",
    "\n",
    "$$ \\Delta x = - \\bigg[ \\frac{\\partial \\nabla f(x)}{x} \\bigg]^{-1} \\nabla f(x_k). $$\n",
    "\n",
    "The jacobian of the gradient of $f(x)$ is the same as the hessian of $f(x)$ (write this out and convince yourself). This means we can rewrite the Newton step as the equivalent expression:\n",
    "\n",
    "$$ \\Delta x = - [\\nabla^2f(x)]^{-1} \\nabla f(x_k) $$\n",
    "\n",
    "What is the interpretation of this? Well, if we take a second order Taylor series of our cost function, and minimize this quadratic approximation of our cost function, we get the following optimization problem:\n",
    "\n",
    "$$ \\min_{\\Delta x} \\quad \\quad f(x_k) + [\\nabla f(x_k)^T] \\Delta x + \\frac{1}{2} \\Delta x^T [\\nabla^2f(x_k)] \\Delta x $$\n",
    "\n",
    "Where our optimality condition is the following:\n",
    "\n",
    "$$ \\nabla f(x_k)^T +  [\\nabla^2f(x_k)] \\Delta x = 0 $$ \n",
    "\n",
    "And we can solve for $\\Delta x$ with the following:\n",
    "\n",
    "$$ \\Delta x = - [\\nabla^2f(x)]^{-1} \\nabla f(x_k) $$\n",
    "\n",
    "Which is our Newton step. This means that Newton's method on the stationary condition is the same as minimizing the quadratic approximation of the cost function at each iteration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
